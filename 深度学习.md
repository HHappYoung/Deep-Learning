# 循环神经网络

[循环神经网络](D:\Assets\课业\大三下\数据挖掘与最优化\数据挖掘与最优化笔记.md)







# Transformer

输入是三维的（batch_size, sequence_length, feature_num)，sequence_length是句子的长度，feature_num是词向量的长度，即用一个多少维的向量去表示一个词。

<img src="md-images/Transformer architecture.png" style="zoom:30%" />




### 3.2 Attention机制

An attention function can be described as mapping a query and a set of key-value pairs to an output,

where the query, keys, values, and output are all vectors. The output is computed as a weighted sum

of the values, where the weight assigned to each value is computed by a compatibility function of the

query with the corresponding key.

query可以理解为你输入的查询语句，key来自词典库，通过一个相似度函数可以计算你输入的query和词典库中的每个key的相似度。output是value的加权和，权重=每个value对应的key和query的相似度。

#### 3.2.1 Scaled Dot-Product Attention

query和key的长度相同，设为 $d_k$ , value的长度设为 $d_v$ 。Scaled Dot-Product Attention就是用query和key做内积（假设有n个key-value对，那么就可以算出来n个内积），然后除以 $\sqrt {d_k}$ ，再施加一个softmax函数，得到的值作为相似度，也即权重。

在实际的计算中，是将多个query放入一个矩阵 $Q$，key也放入一个矩阵 $K$，value也放入一个矩阵 $V$，然后做矩阵乘法。如果有n个query，m个key，则 $Q$ 的维度是 $(n, d_k)$，$K$ 的维度是 $(m,n_k)$，$V$ 的维度是 $(m,d_v)$。最终得到一个 $(n,d_v)$ 的输出。
$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$


#### 3.2.3 Applications of Attention in our Model

Encoder中的注意力机制：可以看到输入分成了3个箭头指向Multi-Head Attention，这3个箭头分别表示key, value, query